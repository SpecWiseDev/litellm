# =============================================================================
# LiteLLM Proxy Configuration for Multi-Product Gateway
# =============================================================================
#
# Providers:
#   - Vercel AI Gateway (primary)
#   - OpenRouter (fallback)
#   - Replicate (open-source models)
#
# Start locally:
#   litellm --config imqa_config.yaml --port 4000
#
# Railway deployment:
#   litellm --config imqa_config.yaml --port $PORT
#
# =============================================================================

model_list:
  # ===========================================================================
  # TIER 1: High-Capability Models (with cascading fallbacks)
  # ===========================================================================
  # When you request "gemini-3-pro", LiteLLM will try providers in order

  # Primary: Google Gemini 3 Pro via Vercel AI Gateway
  - model_name: gemini-3-pro
    litellm_params:
      model: vercel_ai_gateway/google/gemini-3-pro-preview
      api_key: os.environ/VERCEL_AI_GATEWAY_API_KEY
      timeout: 120
      stream_timeout: 60

  # Fallback 1: Same model via OpenRouter
  - model_name: gemini-3-pro
    litellm_params:
      model: openrouter/google/gemini-3-pro-preview
      api_key: os.environ/OPENROUTER_API_KEY
      timeout: 120
      stream_timeout: 60

  # Fallback 2: GPT-4o as alternative
  - model_name: gemini-3-pro
    litellm_params:
      model: vercel_ai_gateway/openai/gpt-5.2
      api_key: os.environ/VERCEL_AI_GATEWAY_API_KEY
      timeout: 120
      stream_timeout: 60

  # ---------------------------------------------------------------------------
  # GPT-4o with fallbacks
  # ---------------------------------------------------------------------------
  - model_name: gpt-5.2
    litellm_params:
      model: vercel_ai_gateway/openai/gpt-5.2
      api_key: os.environ/VERCEL_AI_GATEWAY_API_KEY
      timeout: 120
      stream_timeout: 60

  - model_name: gpt-5.2
    litellm_params:
      model: openrouter/openai/gpt-5.2
      api_key: os.environ/OPENROUTER_API_KEY
      timeout: 120
      stream_timeout: 60

  # ---------------------------------------------------------------------------
  # Claude 4 Sonnet with fallbacks
  # ---------------------------------------------------------------------------
  - model_name: claude-4-sonnet
    litellm_params:
      model: vercel_ai_gateway/anthropic/claude-4-sonnet
      api_key: os.environ/VERCEL_AI_GATEWAY_API_KEY
      timeout: 120
      stream_timeout: 60

  - model_name: claude-4-sonnet
    litellm_params:
      model: openrouter/anthropic/claude-4-sonnet
      api_key: os.environ/OPENROUTER_API_KEY
      timeout: 120
      stream_timeout: 60

  # ===========================================================================
  # TIER 2: Fast/Cheap Models (with fallbacks)
  # ===========================================================================

  # Fast model alias - will try in order
  - model_name: fast-model
    litellm_params:
      model: vercel_ai_gateway/google/gemini-2.5-flash
      api_key: os.environ/VERCEL_AI_GATEWAY_API_KEY
      timeout: 60
      stream_timeout: 30

  - model_name: fast-model
    litellm_params:
      model: vercel_ai_gateway/openai/gpt-4o-mini
      api_key: os.environ/VERCEL_AI_GATEWAY_API_KEY
      timeout: 60
      stream_timeout: 30

  - model_name: fast-model
    litellm_params:
      model: openrouter/google/gemini-2.5-flash
      api_key: os.environ/OPENROUTER_API_KEY
      timeout: 60
      stream_timeout: 30

  # ---------------------------------------------------------------------------
  # GPT-4o-mini with fallbacks
  # ---------------------------------------------------------------------------
  - model_name: gpt-4o-mini
    litellm_params:
      model: vercel_ai_gateway/openai/gpt-4o-mini
      api_key: os.environ/VERCEL_AI_GATEWAY_API_KEY
      timeout: 60
      stream_timeout: 30

  - model_name: gpt-4o-mini
    litellm_params:
      model: openrouter/openai/gpt-4o-mini
      api_key: os.environ/OPENROUTER_API_KEY
      timeout: 60
      stream_timeout: 30

  # ---------------------------------------------------------------------------
  # Gemini Flash with fallbacks
  # ---------------------------------------------------------------------------
  - model_name: gemini-2.5-flash
    litellm_params:
      model: vercel_ai_gateway/google/gemini-2.5-flash
      api_key: os.environ/VERCEL_AI_GATEWAY_API_KEY
      timeout: 60
      stream_timeout: 30

  - model_name: gemini-2.5-flash
    litellm_params:
      model: openrouter/google/gemini-2.5-flash
      api_key: os.environ/OPENROUTER_API_KEY
      timeout: 60
      stream_timeout: 30

  # ===========================================================================
  # TIER 3: Reasoning Models
  # ===========================================================================

  - model_name: reasoning-model
    litellm_params:
      model: vercel_ai_gateway/xai/grok-4.1-fast-reasoning
      api_key: os.environ/VERCEL_AI_GATEWAY_API_KEY
      timeout: 180
      stream_timeout: 90

  - model_name: reasoning-model
    litellm_params:
      model: openrouter/xai/grok-4.1-fast-reasoning
      api_key: os.environ/OPENROUTER_API_KEY
      timeout: 180
      stream_timeout: 90

  - model_name: reasoning-model
    litellm_params:
      model: vercel_ai_gateway/openai/o1-mini
      api_key: os.environ/VERCEL_AI_GATEWAY_API_KEY
      timeout: 180
      stream_timeout: 90

  # ===========================================================================
  # DIRECT ACCESS: Specific models without fallbacks
  # ===========================================================================

  # Anthropic Claude 4 Opus (expensive, no fallback)
  - model_name: claude-4-opus
    litellm_params:
      model: vercel_ai_gateway/anthropic/claude-4-opus
      api_key: os.environ/VERCEL_AI_GATEWAY_API_KEY
      timeout: 180
      stream_timeout: 90

  # Claude 3.5 Sonnet
  - model_name: claude-3.5-sonnet
    litellm_params:
      model: vercel_ai_gateway/anthropic/claude-3.5-sonnet
      api_key: os.environ/VERCEL_AI_GATEWAY_API_KEY
      timeout: 120
      stream_timeout: 60

  - model_name: claude-3.5-sonnet
    litellm_params:
      model: openrouter/anthropic/claude-3.5-sonnet
      api_key: os.environ/OPENROUTER_API_KEY
      timeout: 120
      stream_timeout: 60

  # GPT-4.1 models
  - model_name: gpt-4.1
    litellm_params:
      model: vercel_ai_gateway/openai/gpt-4.1
      api_key: os.environ/VERCEL_AI_GATEWAY_API_KEY
      timeout: 120
      stream_timeout: 60

  - model_name: gpt-4.1-mini
    litellm_params:
      model: vercel_ai_gateway/openai/gpt-4.1-mini
      api_key: os.environ/VERCEL_AI_GATEWAY_API_KEY
      timeout: 60
      stream_timeout: 30

  # Gemini 2.5 Pro
  - model_name: gemini-2.5-pro
    litellm_params:
      model: vercel_ai_gateway/google/gemini-2.5-pro
      api_key: os.environ/VERCEL_AI_GATEWAY_API_KEY
      timeout: 120
      stream_timeout: 60

  - model_name: gemini-2.5-pro
    litellm_params:
      model: openrouter/google/gemini-2.5-pro
      api_key: os.environ/OPENROUTER_API_KEY
      timeout: 120
      stream_timeout: 60

  # xAI Grok models
  - model_name: grok-4-fast
    litellm_params:
      model: vercel_ai_gateway/xai/grok-4-fast
      api_key: os.environ/VERCEL_AI_GATEWAY_API_KEY
      timeout: 120
      stream_timeout: 60

  - model_name: grok-4.1-fast-reasoning
    litellm_params:
      model: vercel_ai_gateway/xai/grok-4.1-fast-reasoning
      api_key: os.environ/VERCEL_AI_GATEWAY_API_KEY
      timeout: 180
      stream_timeout: 90

  # ===========================================================================
  # WILDCARD ROUTES: Catch-all for direct provider access
  # ===========================================================================
  # Use these for any model not explicitly defined above
  # Format: provider/model-name -> routed to appropriate gateway

  # Google models via Vercel AI Gateway
  - model_name: "google/*"
    litellm_params:
      model: "vercel_ai_gateway/google/*"
      api_key: os.environ/VERCEL_AI_GATEWAY_API_KEY
      timeout: 120
      stream_timeout: 60

  # OpenAI models via Vercel AI Gateway
  - model_name: "openai/*"
    litellm_params:
      model: "vercel_ai_gateway/openai/*"
      api_key: os.environ/VERCEL_AI_GATEWAY_API_KEY
      timeout: 120
      stream_timeout: 60

  # Anthropic models via Vercel AI Gateway
  - model_name: "anthropic/*"
    litellm_params:
      model: "vercel_ai_gateway/anthropic/*"
      api_key: os.environ/VERCEL_AI_GATEWAY_API_KEY
      timeout: 120
      stream_timeout: 60

  # xAI models via Vercel AI Gateway
  - model_name: "xai/*"
    litellm_params:
      model: "vercel_ai_gateway/xai/*"
      api_key: os.environ/VERCEL_AI_GATEWAY_API_KEY
      timeout: 120
      stream_timeout: 60

  # Alternative xAI prefix
  - model_name: "x-ai/*"
    litellm_params:
      model: "vercel_ai_gateway/xai/*"
      api_key: os.environ/VERCEL_AI_GATEWAY_API_KEY
      timeout: 120
      stream_timeout: 60

  # OpenRouter direct access (for any model on OpenRouter)
  - model_name: "openrouter/*"
    litellm_params:
      model: "openrouter/*"
      api_key: os.environ/OPENROUTER_API_KEY
      timeout: 120
      stream_timeout: 60

  # Replicate models (open-source, image generation)
  - model_name: "replicate/*"
    litellm_params:
      model: "replicate/*"
      api_key: os.environ/REPLICATE_API_KEY
      timeout: 300  # Replicate can have cold starts
      stream_timeout: 120

# =============================================================================
# LiteLLM Settings
# =============================================================================

litellm_settings:
  # Drop unsupported parameters instead of erroring
  drop_params: true

  # Retry configuration
  num_retries: 3
  request_timeout: 180

  # Disable telemetry
  telemetry: false

  # Redis caching - reduces costs and latency
  cache: true
  cache_params:
    type: redis
    host: os.environ/REDIS_HOST
    port: os.environ/REDIS_PORT
    password: os.environ/REDIS_PASSWORD
    ttl: 3600  # 1 hour cache TTL
    # Optional: Enable semantic caching for similar queries
    # supported_call_types: ["acompletion", "completion"]

  # Prometheus metrics for monitoring
  success_callback: ["prometheus"]

  # Optional: Langfuse for detailed observability
  # success_callback: ["prometheus", "langfuse"]
  # langfuse_public_key: os.environ/LANGFUSE_PUBLIC_KEY
  # langfuse_secret: os.environ/LANGFUSE_SECRET_KEY

# =============================================================================
# Router Settings
# =============================================================================

router_settings:
  # Load balancing strategy
  # - usage-based-routing-v2: Distributes based on usage/cost
  # - simple-shuffle: Random distribution
  # - least-busy: Route to least busy model
  routing_strategy: usage-based-routing-v2

  # Enable pre-call health checks
  enable_pre_call_checks: true

  # Redis for router state (uses same Redis as cache)
  redis_host: os.environ/REDIS_HOST
  redis_port: os.environ/REDIS_PORT
  redis_password: os.environ/REDIS_PASSWORD

# =============================================================================
# General Settings
# =============================================================================

general_settings:
  # Master API key for authentication
  master_key: os.environ/LITELLM_MASTER_KEY

  # Database for usage tracking and virtual keys
  database_url: os.environ/DATABASE_URL
  store_model_in_db: true

  # Health check settings
  # background_health_checks: true
  # health_check_interval: 60

  # Rate limiting (global)
  # Note: Per-key limits can be set via virtual keys in database
  # global_max_parallel_requests: 100
